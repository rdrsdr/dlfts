{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup google-drive mounting (optional)"
      ],
      "metadata": {
        "id": "erlrIYB0q5v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import os\n",
        "\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4Z3V2jwlpNgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: create a symbolic link to a google drive workdir 'xyz' to the root of colab\n",
        "\n",
        "# # Specify the path to your folder\n",
        "# gdrive = '/content/drive/MyDrive/AI/2025'\n",
        "# workdir = '/datasets'\n",
        "# slink = '/content' + workdir\n",
        "# fullpath = gdrive + workdir\n",
        "\n",
        "# # Check if the folder exists\n",
        "# if os.path.exists(fullpath):\n",
        "#   # Create the symbolic link\n",
        "#   try:\n",
        "#     os.symlink(fullpath, slink)\n",
        "#     print(f\"Symbolic link created from '{fullpath}' to '{slink}'\")\n",
        "#   except FileExistsError:\n",
        "#     print(f\"Symbolic link '{slink}' already exists.\")\n",
        "#   except OSError as e:\n",
        "#     print(f\"Error creating symbolic link: {e}\")\n",
        "# else:\n",
        "#   print(f\"Error: Folder '{fullpath}' not found.\")"
      ],
      "metadata": {
        "id": "DZaxie5Coy46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!unzip datasets/dataset.zip -d datasets/\n",
        "#!rm datasets/dataset.zip"
      ],
      "metadata": {
        "id": "DeT28VMlqyZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Notebook Start"
      ],
      "metadata": {
        "id": "yeq-lMpVrA4Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_TzATjB-L2c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "folder = './datasets/crypto/hourly/'\n",
        "\n",
        "df = pd.read_csv(folder + 'dataset.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_bckp = df\n",
        "# df = df_bckp.iloc[17:].reset_index(drop=True)\n",
        "# df"
      ],
      "metadata": {
        "id": "ey8YQTvkakvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "date_column_name = df.columns[0]\n",
        "date_format = '%Y-%m-%d' if date_column_name.lower() == 'date' else '%Y-%m-%d %H:%M:%S'\n",
        "\n",
        "date_column_name, date_format"
      ],
      "metadata": {
        "id": "n_tpyIVT3leg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converter a coluna para datetime removendo o fuso horário\n",
        "if date_column_name.lower() == 'date':\n",
        "    df[date_column_name] = pd.to_datetime(df[date_column_name])\n",
        "else:\n",
        "    df[date_column_name] = pd.to_datetime(df[date_column_name]).dt.tz_convert(None)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "fguoQooUAWlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: plot df using plotly\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.line(df.drop('Real', axis=1), x=date_column_name, y=df.columns[2:])\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "UN7ADtMkzHVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YHhQhQIoIGd"
      },
      "outputs": [],
      "source": [
        "split_date = pd.to_datetime('2024-08-26')\n",
        "#split_date = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Find the first row where the date is equal or greater than split_date\n",
        "def get_split_date_index(df, split_date):\n",
        "  for i in range(len(df)):\n",
        "    if df.iloc[i, 0] >= split_date:\n",
        "      return i\n",
        "\n",
        "split_idx = get_split_date_index(df, split_date)\n",
        "split_idx, df.iloc[split_idx, 0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_idx += 17 # adjusting for other forecasts"
      ],
      "metadata": {
        "id": "Vod13TdGf80z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df) - split_idx"
      ],
      "metadata": {
        "id": "Xb0bM3eSxKp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAUIJxqi-L2c"
      },
      "outputs": [],
      "source": [
        "# needed for dumb4cast\n",
        "df_train = df.iloc[:split_idx, :].drop('Real', axis=1)\n",
        "df_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1AcYR2s-L2c"
      },
      "outputs": [],
      "source": [
        "# needed for dumb4cast\n",
        "df_test = df.iloc[split_idx:, :].drop('Real', axis=1)\n",
        "df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4TNMwYHQRAX"
      },
      "outputs": [],
      "source": [
        "# loading pre-trained model\n",
        "MODEL_NAMES = [\n",
        "    # 'arima',\n",
        "    'lstm',\n",
        "    'gru',\n",
        "    'mlp',\n",
        "    'dlinear',\n",
        "    'nlinear',\n",
        "    'informer',\n",
        "    'autoformer',\n",
        "    'fedformer',\n",
        "    'bitcn',\n",
        "    'rnn',\n",
        "\n",
        "    'tcn',\n",
        "    'deepar',\n",
        "    'dilatedrnn',\n",
        "    #'nbeats',\n",
        "    #'nbeatsx',\n",
        "    'nhits',\n",
        "    'tide',\n",
        "    'deepnpts',\n",
        "    'tft',\n",
        "    'vanilla',\n",
        "    'patchtst',\n",
        "    #'itransformer',\n",
        "    #'timesnet'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T3sXPQ90mEe"
      },
      "outputs": [],
      "source": [
        "# prompt: add last row of df_train to a new test_df dataframe\n",
        "# Por que: o backtest precisa comparar os dados reais do dia anterior à previsão pra saber se compra ou vende\n",
        "real_column = df['Real'][split_idx-1:]\n",
        "test_df = df.iloc[split_idx-1:, :].drop('Real', axis=1)\n",
        "print(real_column)\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLn00bZQ-L2d"
      },
      "outputs": [],
      "source": [
        "# forecasts without cross-validation\n",
        "forecasts = []\n",
        "\n",
        "forecasts_path = 'forecasts/'\n",
        "path = folder + forecasts_path\n",
        "\n",
        "for model_name in MODEL_NAMES:\n",
        "  print(f'Loading {model_name.upper()} forecasts')\n",
        "  forecast_df = pd.read_csv(path + 'forecast-' + model_name + '.csv')\n",
        "\n",
        "  forecast_df = forecast_df.iloc[17:] # adjusting for cross validation forecasts\n",
        "\n",
        "  forecasts.append(forecast_df[test_df.columns])\n",
        "\n",
        "forecasts[0][test_df.columns].head(5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_crossvalidation_models(path, model_names, columns_order, debug = False, order = False):\n",
        "  forecasts = []\n",
        "  for model_name in model_names:\n",
        "    if debug:\n",
        "      print(f'Loading {model_name.upper()} forecasts')\n",
        "    forecast_df = pd.read_csv(path + model_name + '.csv')\n",
        "    forecast_df = forecast_df.rename(columns={'ds': 'Datetime'})\n",
        "    forecast_df = forecast_df.drop(['cutoff', 'y'], axis=1)\n",
        "    forecast_df = forecast_df.pivot(index='Datetime', columns='unique_id', values=forecast_df.columns[2])\n",
        "    forecast_df = forecast_df.reset_index()\n",
        "    if (order):\n",
        "      forecast_df = forecast_df[columns_order]\n",
        "\n",
        "    forecasts.append(forecast_df)\n",
        "  if debug:\n",
        "    display(forecast_df)\n",
        "  return forecasts"
      ],
      "metadata": {
        "id": "7DQ7a43PIGci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forecasts with cross-validation (no refit)\n",
        "cross_plain_path = folder + 'cross-validation/plain/'\n",
        "\n",
        "plain_forecasts = load_crossvalidation_models(cross_plain_path + forecasts_path, MODEL_NAMES, test_df.columns, True, True)"
      ],
      "metadata": {
        "id": "TFvMNJlEVUiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forecasts with cross-validation (no refit)\n",
        "cross_plain_btc_path = folder + 'cross-validation/plain-btc/'\n",
        "\n",
        "plain_btc_forecasts = load_crossvalidation_models(cross_plain_btc_path + forecasts_path, MODEL_NAMES, test_df.columns, True, False)"
      ],
      "metadata": {
        "id": "2YDH4SjdTBGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forecasts with cross-validation and refit\n",
        "cross_refit_path = folder + 'cross-validation/refit/'\n",
        "\n",
        "refit_forecasts = load_crossvalidation_models(cross_refit_path + forecasts_path, MODEL_NAMES, test_df.columns, True, True)"
      ],
      "metadata": {
        "id": "lwdz1IhbT91z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQTOkbex9_Vu"
      },
      "outputs": [],
      "source": [
        "# creating a baseline buy-and-hold dataframe\n",
        "\n",
        "IB = 100.0 # initial balance for each asset\n",
        "\n",
        "# copy test_df to bh_df and set every value to zero\n",
        "bh_df = test_df.copy()\n",
        "for col in bh_df.columns[1:]:\n",
        "  bh_df[col] = 0\n",
        "bh_df[bh_df.columns[0]] = test_df[test_df.columns[0]]\n",
        "\n",
        "# except from the first column of bh_df, divide every other first row's value by IB (buying stocks)\n",
        "for j in range(1, len(test_df.columns)):\n",
        "  bh_df.iloc[0, j] = IB#test_df.iloc[0, j] / IB\n",
        "\n",
        "# calculating how much each stock would worth every day\n",
        "for i in range(1, len(test_df)):\n",
        "  for j in range(1, len(test_df.columns)):\n",
        "    bh_df.iloc[i, j] = bh_df.iloc[i-1, j] * (test_df.iloc[i, j] / test_df.iloc[i-1, j])\n",
        "\n",
        "bh_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "bh_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Tbqkml6_Vs3"
      },
      "outputs": [],
      "source": [
        "# it looks all models prediction are just an one day shit from the real values.\n",
        "# dumb_4cast is a simple one day shit from test-data, so it'll force a buy transcation if the current\n",
        "# value had an increase compared to the previous day or sell if there was a drop of it's value.\n",
        "\n",
        "dumb_4cast = df_train.tail(2).copy()\n",
        "dumb_4cast = pd.concat([dumb_4cast, df_test[:-1]], ignore_index=True)\n",
        "dumb_4cast.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# prompt: subtract one day for every value of column Date from dumb_4cast\n",
        "# convert it first\n",
        "dumb_4cast[date_column_name] = pd.to_datetime(dumb_4cast[date_column_name])\n",
        "\n",
        "if date_column_name.lower() == 'date':\n",
        "    dumb_4cast[date_column_name] = dumb_4cast[date_column_name] + pd.DateOffset(days=1)\n",
        "else:\n",
        "    dumb_4cast[date_column_name] = dumb_4cast[date_column_name] + pd.DateOffset(hours=1)\n",
        "\n",
        "# prompt: change dumb_4cast Date column back to string type\n",
        "dumb_4cast[date_column_name] = dumb_4cast[date_column_name].dt.strftime(date_format)\n",
        "\n",
        "dumb_4cast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAMES.append('dumb_4cast')\n",
        "forecasts.append(dumb_4cast)"
      ],
      "metadata": {
        "id": "iGuULhmbNtwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yiGaMqmeOuy"
      },
      "source": [
        "Creating a dataframe with a wallet composed by the models predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: count how many True values are in real_column\n",
        "true_count = sum(real_column)\n",
        "print(f\"Number of True values in real_column: {true_count}\")"
      ],
      "metadata": {
        "id": "ISmZ8qg5DD2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backtest(real_df, real_column, pred_df):\n",
        "  # copy real_df to model_df and set every value to zero\n",
        "  model_df = real_df.copy()\n",
        "  for col in model_df.columns[1:]:\n",
        "    model_df[col] = 0\n",
        "  model_df[model_df.columns[0]] = real_df[real_df.columns[0]]\n",
        "\n",
        "  model_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  # except from the first column of model_df, divide every other first row's value by IB (buying stocks)\n",
        "  for j in range(1, len(real_df.columns)):\n",
        "    model_df.iloc[0, j] = IB\n",
        "\n",
        "  count = 0\n",
        "\n",
        "  # calculating how much each stock would worth every day\n",
        "  for i in range(1, len(real_df)):\n",
        "    if not real_column.iloc[i]: # don't backtest if market is closed (filled transaction, not real)\n",
        "      # copy previous row\n",
        "      model_df.iloc[i, 1:] = model_df.iloc[i-1, 1:]\n",
        "    else:\n",
        "      count += 1\n",
        "      for j in range(1, len(real_df.columns)):\n",
        "        if pred_df.iloc[i-1, j] > real_df.iloc[i-1, j]: # predicted value is higher than current value: buy\n",
        "          x = model_df.iloc[i-1, j] * (real_df.iloc[i, j] / real_df.iloc[i-1, j])\n",
        "        else:                                           # predicted value is lower than current value: sell\n",
        "          x = model_df.iloc[i-1, j] * (real_df.iloc[i-1, j] / real_df.iloc[i, j])\n",
        "\n",
        "        if x > 0:\n",
        "          model_df.iloc[i, j] = x\n",
        "        else:\n",
        "          model_df.iloc[i, j] = 0\n",
        "          break\n",
        "\n",
        "  print(f'\\t{count} predicted steps')\n",
        "\n",
        "  return model_df\n",
        "\n",
        "# i = 0\n",
        "# print(f'backtesting {MODEL_NAMES[i]}: ', end='')\n",
        "# backtest(test_df, real_column, forecasts[i])"
      ],
      "metadata": {
        "id": "qz_TyTps6jj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_backtests(model_names, test_df, real_column, forecasts, backtests_df):\n",
        "  for i in range(len(model_names)):\n",
        "    print(f'backtesting {model_names[i]}: ', end='')\n",
        "    bt_df = backtest(test_df, real_column, forecasts[i])\n",
        "    backtests_df.append(bt_df)\n",
        "\n",
        "  return backtests_df"
      ],
      "metadata": {
        "id": "NBbWRcnw5EZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZizHvf1ufxP"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "backtests = [bh_df]\n",
        "\n",
        "# for i in range(len(MODEL_NAMES)):\n",
        "#   print(f'backtesting {MODEL_NAMES[i]}: ', end='')\n",
        "#   model_rev_df = backtest(test_df, real_column, forecasts[i])\n",
        "#   backtests.append(model_rev_df)\n",
        "\n",
        "_ = run_backtests(MODEL_NAMES, test_df, real_column, forecasts, backtests)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape, real_column.shape, forecasts[0].shape, backtests[0].shape"
      ],
      "metadata": {
        "id": "WJQh2dKvSCFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.shape, real_column.shape, plain_forecasts[0].shape"
      ],
      "metadata": {
        "id": "vp_bDiiFsdhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAMES.remove('dumb_4cast')"
      ],
      "metadata": {
        "id": "_eqj4oVSu8T-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "plain_backtests = []\n",
        "\n",
        "_ = run_backtests(MODEL_NAMES, test_df, real_column, plain_forecasts, plain_backtests)"
      ],
      "metadata": {
        "id": "E6gNQgug5rFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "plain_btc_backtests = []\n",
        "\n",
        "_ = run_backtests(MODEL_NAMES, test_df.iloc[:, 0:2], real_column, plain_btc_forecasts, plain_btc_backtests)"
      ],
      "metadata": {
        "id": "CHihAG-ve9Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "refit_backtests = []\n",
        "\n",
        "_ = run_backtests(MODEL_NAMES, test_df, real_column, refit_forecasts, refit_backtests)"
      ],
      "metadata": {
        "id": "9KL9tH49IKVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# folder # './datasets/crypto/hourly/'\n",
        "# path # folder + 'forecasts/' # forecasts without cross-validation\n",
        "# cross_plain_path # folder + 'cross-validation/plain/' # forecasts with cross-validation (no refit)\n",
        "# cross_refit_path # folder + 'cross-validation/refit/' # forecasts with cross-validation and refit\n",
        "\n",
        "# df # original dataset loaded from .csv\n",
        "\n",
        "# date_column_name # 'Datetime' or 'Date'\n",
        "# date_format # '%Y-%m-%d' or '%Y-%m-%d %H:%M:%S'\n",
        "# split_idx # 11616 (index where training and testing datasets are split, separated by provided date)\n",
        "\n",
        "# df_train # df up to split_idx\n",
        "# df_test # df from split_idx\n",
        "# real_column # column 'Real' values extracted from df\n",
        "# test_df # df from split_idx-1 dropped by column 'Real'\n",
        "\n",
        "# MODEL_NAMES # list of all used models (appended by 'dumb_4cast' later)\n",
        "\n",
        "# forecasts # forecasts without cross-validation\n",
        "# plain_forecasts # forecasts with cross-validation (no refit)\n",
        "# refit_forecasts # forecasts with cross-validation and refit\n",
        "\n",
        "# IB # 100.0 initial balance for each asset\n",
        "\n",
        "# bh_df # baseline buy-and-hold dataframe\n",
        "# dumb_4cast # simple one day shit from df_train\n",
        "\n",
        "# backtests # backtests calculated from forecasts\n",
        "# plain_backtests # backtests calculated from plain_forecasts\n",
        "# refit_backtests # backtests calculated from refit_forecasts"
      ],
      "metadata": {
        "id": "qAF3my52lfT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folder, path, cross_plain_path, cross_refit_path"
      ],
      "metadata": {
        "id": "PnWtMtqrdJpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: save all models in a 'backtest' folder inside workdir\n",
        "import os\n",
        "\n",
        "def save_backtests(folder, model_names, backtests):\n",
        "  # Create the 'backtest' directory if it doesn't exist\n",
        "  backtest_dir = os.path.join(folder, 'backtests')  # Use workdir variable\n",
        "  os.makedirs(backtest_dir, exist_ok=True)\n",
        "\n",
        "  # Save buy and hold and dumb4cast to the backtest folder\n",
        "  # bh_filename = os.path.join(backtest_dir, \"buy-and-hold.csv\")\n",
        "  # bh_df.to_csv(bh_filename, index=False)\n",
        "\n",
        "  # Save each model's dataframe to a separate CSV file in the 'backtest' folder\n",
        "  for i, model_name in enumerate(model_names):\n",
        "      filename = os.path.join(backtest_dir, f\"{model_name}.csv\")\n",
        "      backtests[i].to_csv(filename, index=False)  # i+1 to skip bh_df\n",
        "\n",
        "  print(f\"Backtest results saved to '{backtest_dir}'\")"
      ],
      "metadata": {
        "id": "yQGE7nsfHF12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_backtests(cross_plain_path, MODEL_NAMES, plain_backtests)\n",
        "save_backtests(cross_plain_btc_path, MODEL_NAMES, plain_btc_backtests)\n",
        "save_backtests(cross_refit_path, MODEL_NAMES, refit_backtests)"
      ],
      "metadata": {
        "id": "o4VGSVyRjA7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qykfIFbLV8K"
      },
      "outputs": [],
      "source": [
        "import IPython\n",
        "IPython.display.Audio(\"file_example_MP3_1MG.mp3\", autoplay=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AgiqccI-L2e"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "erlrIYB0q5v_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}